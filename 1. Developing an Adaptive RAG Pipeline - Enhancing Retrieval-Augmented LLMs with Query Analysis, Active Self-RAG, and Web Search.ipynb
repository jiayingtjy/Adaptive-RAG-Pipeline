{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb3c708",
   "metadata": {},
   "source": [
    "<h1 style=\"\n",
    "    color: #f09; \n",
    "    background-color: #ffe5ec; \n",
    "    font-family: serif; \n",
    "    text-align: center; \n",
    "    padding: 10px; \n",
    "    border-radius: 5px;\n",
    "    box-shadow: 0px 3px 8px rgba(0, 0, 0, 0.3);\n",
    "    margin-top: 20px;\n",
    "\">\n",
    "    Developing an Adaptive RAG Pipeline - Enhancing Retrieval-Augmented LLMs with Query Analysis, Active Self-RAG, and Web Search\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231198f",
   "metadata": {},
   "source": [
    "<h2 style=\"\n",
    "    color: #fb6f92; \n",
    "    background-color: #0a0a0a; \n",
    "    font-family: serif;\n",
    "    text-align: center; \n",
    "    padding: 15px; \n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0px 4px\n",
    "           10px rgba(0, 0, 0, 0.7); \n",
    "    width: 100%; \n",
    "    margin: 20px auto;\n",
    "\">\n",
    "    1. Project Introduction\n",
    "</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5afcaed0-3d55-4e1f-95d3-c32c751c29d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "![Alt text](Adaptive_RAG_Pipeline_Architechure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e127a0",
   "metadata": {},
   "source": [
    "<h1>Adaptive Retrieval-Augmented Generation (RAG) Pipeline</h1>\n",
    "<p>This project focuses on building an <strong>Adaptive Retrieval-Augmented Generation (RAG)</strong> pipeline. The strategy combines <strong>(1) Query Analysis</strong> with <strong>(2) Self-RAG</strong>, dynamically tailoring retrieval and generation processes to optimize efficiency and accuracy.</p>\n",
    "\n",
    "<h2 style=\"color:#f09\">Problem Description</h2>\n",
    "<p>While Retrieval-Augmented Generation (RAG) systems enhance response accuracy by combining generative capabilities of large language models (LLMs) with retrieval mechanisms, existing systems face key challenges:</p>\n",
    "<ul>\n",
    "    <li><strong>Handling Simple Queries:</strong> Current systems often apply unnecessary computational overhead to simple queries, making them inefficient.</li>\n",
    "    <li><strong>Addressing Complex Queries:</strong> These systems struggle with multi-step queries, leading to incomplete or suboptimal responses.</li>\n",
    "</ul>\n",
    "<p>Real-world queries exhibit varying complexity levels, necessitating a dynamic and adaptive approach to optimize performance for diverse tasks.</p>\n",
    "\n",
    "<h2 style=\"color:#f09\">Solution</h2>\n",
    "<p>The <strong>Adaptive RAG Pipeline</strong> addresses these challenges by dynamically adjusting retrieval and generation strategies based on query complexity. This ensures:</p>\n",
    "<ul>\n",
    "    <li><strong>Efficiency:</strong> Simple queries are handled with lightweight or no retrieval, minimizing resource usage.</li>\n",
    "    <li><strong>Accuracy:</strong> Complex queries are tackled with iterative retrieval and multi-step reasoning to provide accurate and context-aware responses.</li>\n",
    "    <li><strong>Adaptability:</strong> A query classifier selects the optimal strategy (e.g., Self-RAG or web search) based on query complexity.</li>\n",
    "</ul>\n",
    "\n",
    "<h2 style=\"color:#f09\">Project Flow</h2>\n",
    "<p style=\"color:red\">To implement this pipeline, the project uses <strong>state machines</strong> for constructing and managing diverse RAG flows, leveraging LangGraph for flow engineering. Nodes and edges will represent components of the pipeline, compiled into a cohesive architecture.</p>\n",
    "\n",
    "\n",
    "<h3>Phase 1: Pre-Retrieval</h3>\n",
    "<ul>\n",
    "    <li><strong>Indexing:</strong> Prepare data for efficient retrieval by creating indexed representations of external knowledge bases.</li>\n",
    "    <li><strong>Query Analysis and Routing:</strong> Analyze query complexity and determine routing:\n",
    "        <ul>\n",
    "            <li>If the query is related to the index, route it to <strong>Self-RAG.</strong></li>\n",
    "            <li>If the query is unrelated to the index, route it to <strong>web search.</strong></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h3>Phase 2A: Active Self-RAG</h3>\n",
    "<p>Build nodes and edges for an <strong>Active Self-RAG</strong> process to handle queries related to the indexed data. This involves:</p>\n",
    "<ul>\n",
    "    <li><strong>Active RAG:</strong> Dynamically decides <em>when</em> and <em>what</em> to retrieve, rewrite, or re-retrieve based on query analysis and LLM reasoning.</li>\n",
    "    <li><strong>Self-RAG:</strong> Incorporates self-reflection and self-grading on retrieved documents and generated responses, improving quality through iterative refinement.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Phase 2B: Active Web Search</h3>\n",
    "<p>Build nodes and edges for integrating web search into the pipeline for queries unrelated to the indexed data. Use retrieval and generation strategies tailored to external web-based information.</p>\n",
    "\n",
    "<h3>Phase 3: Graph Construction</h3>\n",
    "<p>The Adaptive RAG pipeline uses a <strong>state graph</strong> to represent the flow of operations. This phase involves:</p>\n",
    "<ul>\n",
    "    <li><strong>Defining Nodes:</strong> Each node represents a specific operation in the pipeline, such as retrieving documents, grading their relevance, or generating an answer.</li>\n",
    "    <li><strong>Adding Edges:</strong> Edges connect nodes and define the sequence of operations. Conditional edges determine the next step based on the output of the current node, allowing for dynamic decision-making.</li>\n",
    "    <li><strong>Workflow Compilation:</strong> All nodes and edges are compiled into a cohesive workflow using LangGraph, which supports flexible flow engineering and dynamic execution.</li>\n",
    "</ul>\n",
    "<p>For example, the pipeline dynamically routes a query through nodes like <strong>retrieve</strong>, <strong>grade_documents</strong>, or <strong>web_search</strong>, depending on the query's complexity and relevance to indexed data.</p>\n",
    "\n",
    "<h3>Phase 4: Graph Usage</h3>\n",
    "<p>In this phase, the constructed graph is executed to process user queries:</p>\n",
    "<ul>\n",
    "    <li><strong>Streaming Execution:</strong> The graph is streamed iteratively, with each node updating the graph's state.</li>\n",
    "    <li><strong>Dynamic Routing:</strong> Conditional edges ensure that queries are routed to the appropriate nodes (e.g., <strong>web_search</strong> for unrelated queries or <strong>generate</strong> for producing answers).</li>\n",
    "    <li><strong>State Updates:</strong> Each node processes the input graph state and produces an updated state, which is passed to the next node.</li>\n",
    "    <li><strong>Final Generation:</strong> The pipeline outputs a comprehensive response to the userâ€™s query, whether retrieved from indexed data or web-based information.</li>\n",
    "</ul>\n",
    "<p>This dynamic execution ensures an efficient and accurate response generation process, adapting seamlessly to the complexity of each query.</p>\n",
    "\n",
    "<h2 style=\"color:#f09\">Usage of RRR Framework</h2>\n",
    "<p>The project incorporates the <strong>Rewriting, Re-ranking, and Re-retrieving (RRR)</strong> framework within the Adaptive RAG pipeline to enhance retrieval and generation processes:</p>\n",
    "<ul>\n",
    "    <li><strong>Rewriting:</strong> The <em>Query Rewriter</em> optimizes queries for better alignment with indexed data or retrieval mechanisms, especially when initial retrieval results are ambiguous or insufficient.</li>\n",
    "    <li><strong>Re-ranking:</strong> The <em>Retrieval Grader</em> evaluates and prioritizes the relevance of retrieved documents, filtering out irrelevant ones to ensure high-quality inputs for answer generation.</li>\n",
    "    <li><strong>Re-retrieving:</strong> The pipeline iteratively refines retrieval using the <em>Transform Query</em> node, which rewrites the question to address gaps in the initial retrieval phase.</li>\n",
    "</ul>\n",
    "<p>By leveraging the RRR framework, the Adaptive RAG pipeline ensures a robust and adaptive retrieval process, balancing efficiency and accuracy for diverse query complexities.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h2 style=\"color:#f09\">Referenced Paper</h2>\n",
    "<p><em>Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</em></p>\n",
    "<p><a href=\"https://arxiv.org/abs/2403.14403\" target=\"_blank\">https://arxiv.org/abs/2403.14403</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb78fa",
   "metadata": {},
   "source": [
    "<h2 style=\"\n",
    "    color: #fb6f92; \n",
    "    background-color: #0a0a0a; \n",
    "    font-family: serif;\n",
    "    text-align: center; \n",
    "    padding: 15px; \n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0px 4px\n",
    "           10px rgba(0, 0, 0, 0.7); \n",
    "    width: 100%; \n",
    "    margin: 20px auto;\n",
    "\">\n",
    "    2. Project Implementation \n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85501ca-eb89-4795-aeab-cdab050ead6b",
   "metadata": {},
   "source": [
    "## 2.1 Setup\n",
    "\n",
    "First, let's install our required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d1a740-9fea-4a6e-8f95-fb9dbf1c80a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "plantcv 3.14.1 requires numpy<1.23,>=1.11, but you have numpy 1.26.4 which is incompatible.\n",
      "langchain-chroma 0.1.4 requires chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0, but you have chromadb 0.6.2 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jiayi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e04b18",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Here we will set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development. We will use LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222f204d-956f-4128-b597-2c698120edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ['TAVILY_API_KEY']= \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fe965",
   "metadata": {},
   "source": [
    "## 2.2 Phase 1: Pre-Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c985b0",
   "metadata": {},
   "source": [
    "<h3>How it works?</h3>\n",
    "\n",
    "<h3 style=\"color:red\">Step 1: Creating Index/Vectorstore</h3>\n",
    "<p>For indexing, we have utilized the <strong>chunk optimization indexing technique</strong> for efficient retrieval.</p>\n",
    "<h3 style=\"color:red\">Step 2: Defining LLMs for Query Analysis and Routing</h3>\n",
    "<p>We use an LLM-based routing mechanism to analyze the query and decide whether to route it to the vectorstore or web search based on the query's content.</p>\n",
    "<h3 style=\"color:red\">Step 3: Constructing the Graph</h3>\n",
    "<p>We capture the flow as a graph, where the graph state includes information about the query, retrieved documents, and any generated responses.</p>\n",
    "\n",
    "<h3 style=\"color:red\">Step 4:  Defining Graph Edges as Functions</h3>\n",
    "<p>The graph edges are defined as functions that determine the flow based on the current graph state.</p>\n",
    "\n",
    "<p>In this phase, the query is analyzed and routed either to the <strong>vectorstore</strong> for retrieval or to <strong>web search</strong> for external information. This pre-retrieval phase ensures optimal resource utilization and accuracy in subsequent steps.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6eed22",
   "metadata": {},
   "source": [
    "### 2.2.1 Step 1: Creating Index/Vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef847a",
   "metadata": {},
   "source": [
    "For indexing techniques, here we have used chunk optimization indexing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eddf3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "### Build Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "### from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embd,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623d04a",
   "metadata": {},
   "source": [
    "### 2.2.2 Step 2: Defining LLMs for Query Analysis and Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213ce93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiaYi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3397: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n",
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(question_router.invoke({\"question\": \"Who will the Bears draft first in the NFL draft?\"}))\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13437e",
   "metadata": {},
   "source": [
    "### 2.2.3 Step 3: Constructing the GraphÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ff6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be capturing the flow as a graph. Hence we first define the graph state.\n",
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637f6f2",
   "metadata": {},
   "source": [
    "### 2.2.4: Step 4: Defining Graph Edges as Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6beb0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges ###\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef0b38",
   "metadata": {},
   "source": [
    "## 2.3 Phase 2A: Active Self-RAG (if query is related to index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94111997",
   "metadata": {},
   "source": [
    "### 2.3.1 Description of Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad39ab0",
   "metadata": {},
   "source": [
    "<h2>Active Self-RAG Pipeline</h2>\n",
    "\n",
    "<h3 style=\"color:red\">Step 1: Building Required LLMs</h3>\n",
    "<p>The following LLMs are essential for the <strong>Active Self-RAG</strong> pipeline, each playing a specific role in retrieval, evaluation, generation, and query refinement:</p>\n",
    "<ol>\n",
    "    <li><strong>Answer Generator</strong>\n",
    "        <ul>\n",
    "            <li><strong>Purpose:</strong> Generate a comprehensive response based on the input question and retrieved documents.</li>\n",
    "            <li><strong>Output:</strong> Generated answer as text.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Retrieval Grader</strong>\n",
    "        <ul>\n",
    "            <li><strong>Purpose:</strong> Evaluate whether a retrieved document is relevant to the userâ€™s query.</li>\n",
    "            <li><strong>Output:</strong> Binary decision (<code>Yes</code> or <code>No</code>).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Hallucination Grader</strong>\n",
    "        <ul>\n",
    "            <li><strong>Purpose:</strong> Assess if the LLM-generated response is grounded in the retrieved facts.</li>\n",
    "            <li><strong>Output:</strong> Binary decision (<code>Yes</code> or <code>No</code>).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Answer Grader</strong>\n",
    "        <ul>\n",
    "            <li><strong>Purpose:</strong> Determine if the LLM-generated response directly addresses the userâ€™s query.</li>\n",
    "            <li><strong>Output:</strong> Binary decision (<code>Yes</code> or <code>No</code>).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Question Rewriter</strong>\n",
    "        <ul>\n",
    "            <li><strong>Purpose:</strong> Improve the phrasing of a previous LLM-generated question to enhance retrieval and generation quality.</li>\n",
    "            <li><strong>Output:</strong> Rewritten question as text.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<h3 style=\"color:red\">Step 2: Chains Construction</h3>\n",
    "<p>The LLMs are encapsulated in chains by combining:</p>\n",
    "<ul>\n",
    "    <li><strong>Prompt templates:</strong> Define input-output formatting.</li>\n",
    "    <li><strong>LLM models:</strong> To process inputs and produce outputs.</li>\n",
    "    <li><strong>StrOutputParser():</strong> For handling string-based outputs.</li>\n",
    "</ul>\n",
    "<p><strong>Constructed Chains:</strong></p>\n",
    "<ol>\n",
    "    <li><strong>answer_generator:</strong> For generating responses.</li>\n",
    "    <li><strong>retrieval_grader:</strong> For filtering irrelevant documents.</li>\n",
    "    <li><strong>hallucination_grader:</strong> For validating grounding of generated answers.</li>\n",
    "    <li><strong>answer_grader:</strong> For checking if the query is addressed.</li>\n",
    "    <li><strong>question_rewriter:</strong> For refining questions.</li>\n",
    "</ol>\n",
    "\n",
    "<h3 style=\"color:red\">Step 3: Nodes Construction</h3>\n",
    "<p>Nodes define functions that take the current graph state and return an updated graph state.</p>\n",
    "<p><strong>Example state structure:</strong></p>\n",
    "<pre>\n",
    "state = {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "</pre>\n",
    "\n",
    "<p><strong>Nodes:</strong></p>\n",
    "<ol>\n",
    "    <li><strong>retrieve(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Uses the retriever (from the index/vector store) to fetch relevant documents for the query.</li>\n",
    "            <li><strong>Updates:</strong> Adds <code>documents</code> to the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>generate(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Passes the <code>documents</code> and <code>question</code> as input to the <strong>Answer Generator</strong> chain to create a response.</li>\n",
    "            <li><strong>Updates:</strong> Adds <code>generation</code> to the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>grade_documents(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Filters out irrelevant documents by evaluating each document using the <strong>Retrieval Grader</strong>.</li>\n",
    "            <li><strong>Updates:</strong> Retains only relevant <code>documents</code> in the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>transform_query(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Uses the <strong>Question Rewriter</strong> to refine the input question.</li>\n",
    "            <li><strong>Updates:</strong> Replaces the <code>question</code> with the rewritten version in the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<h3 style=\"color:red\">Step 4: Router Construction</h3>\n",
    "<p><strong>Routers:</strong></p>\n",
    "<ol>\n",
    "    <li><strong>decide_to_generate:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Input:</strong> Filtered documents from <code>grade_documents</code>.</li>\n",
    "            <li><strong>Conditions:</strong>\n",
    "                <ul>\n",
    "                    <li>If no relevant documents: Return <code>\"transform_query\"</code>.</li>\n",
    "                    <li>If relevant documents exist: Return <code>\"generate\"</code>.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>grade_generation_v_documents_and_question:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Input:</strong> Generated response and retrieved documents.</li>\n",
    "            <li><strong>Conditions:</strong>\n",
    "                <ul>\n",
    "                    <li>If <strong>hallucination_grader</strong> returns <code>Yes</code>:\n",
    "                        <ul>\n",
    "                            <li>Pass to <strong>answer_grader</strong>:\n",
    "                                <ul>\n",
    "                                    <li>If <strong>answer_grader</strong> returns <code>Yes</code>: Return <code>\"useful\"</code> (end pipeline).</li>\n",
    "                                    <li>If <strong>answer_grader</strong> returns <code>No</code>: Return <code>\"not useful\"</code> (route to <code>transform_query</code>).</li>\n",
    "                                </ul>\n",
    "                            </li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                    <li>If <strong>hallucination_grader</strong> returns <code>No</code>: Return <code>\"not supported\"</code> (route back to <code>generate</code> for regeneration).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed9d1d",
   "metadata": {},
   "source": [
    "### 2.3.2 Step 1 & 2: Define LLMs and Constucting chains that needs to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856801cb-f42a-44e7-956f-47845e3664ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JiaYi\\AppData\\Local\\Temp\\ipykernel_12532\\2592496654.py:29: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "### 1.Retrieval Grader\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2272333e-50b2-42ab-b472-e1055a3b94a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent memory includes short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods. The memory module records agents' experiences in natural language and surfaces context to inform behavior based on relevance, recency, and importance. Reflection mechanisms synthesize memories into higher-level inferences over time to guide future behavior.\n"
     ]
    }
   ],
   "source": [
    "### 2. ANSWER GENERATOR\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "question = \"agent memory\"\n",
    "\n",
    "# Chain\n",
    "answer_generator = prompt | llm | StrOutputParser()\n",
    "# Run\n",
    "generation = answer_generator.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c08d14-77a0-4eed-b882-2d636abb22a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 3. Hallucination Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded99680-437a-4c9d-b860-619c88949d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 4. Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d75f1d7-a47a-4577-bb0d-84b504b0867e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the role of memory in an agent's functioning?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 5. Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc63584",
   "metadata": {},
   "source": [
    "### 2.3.3 Step 3: Define graph nodes using functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b83d00",
   "metadata": {},
   "source": [
    "<p><strong>Graph Nodes Defined:</strong></p>\n",
    "<ol>\n",
    "    <li><strong>retrieve(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Uses the retriever (from the index/vector store) to fetch relevant documents for the query.</li>\n",
    "            <li><strong>Updates:</strong> Adds <code>documents</code> to the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>generate(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Passes the <code>documents</code> and <code>question</code> as input to the <strong>Answer Generator</strong> chain to create a response.</li>\n",
    "            <li><strong>Updates:</strong> Adds <code>generation</code> to the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>grade_documents(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Filters out irrelevant documents by evaluating each document using the <strong>Retrieval Grader</strong>.</li>\n",
    "            <li><strong>Updates:</strong> Retains only relevant <code>documents</code> in the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>transform_query(state):</strong>\n",
    "        <ul>\n",
    "            <li><strong>Functionality:</strong> Uses the <strong>Question Rewriter</strong> to refine the input question.</li>\n",
    "            <li><strong>Updates:</strong> Replaces the <code>question</code> with the rewritten version in the state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b5cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = answer_generator.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac4eaf",
   "metadata": {},
   "source": [
    "### 2.3.4 Step 4: Define routers (using functions) for graph conditional edges "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743370d7",
   "metadata": {},
   "source": [
    "<h3>Routers Construction</h3>\n",
    "<p><strong>Router:</strong></p>\n",
    "<ol>\n",
    "    <li><strong>decide_to_generate:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Input:</strong> Filtered documents from <code>grade_documents</code>.</li>\n",
    "            <li><strong>Conditions:</strong>\n",
    "                <ul>\n",
    "                    <li>If no relevant documents: Return <code>\"transform_query\"</code>.</li>\n",
    "                    <li>If relevant documents exist: Return <code>\"generate\"</code>.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>grade_generation_v_documents_and_question:</strong>\n",
    "        <ul>\n",
    "            <li><strong>Input:</strong> Generated response and retrieved documents.</li>\n",
    "            <li><strong>Conditions:</strong>\n",
    "                <ul>\n",
    "                    <li>If <strong>hallucination_grader</strong> returns <code>Yes</code>:\n",
    "                        <ul>\n",
    "                            <li>Pass to <strong>answer_grader</strong>:\n",
    "                                <ul>\n",
    "                                    <li>If <strong>answer_grader</strong> returns <code>Yes</code>: Return <code>\"useful\"</code> (end pipeline).</li>\n",
    "                                    <li>If <strong>answer_grader</strong> returns <code>No</code>: Return <code>\"not useful\"</code> (route to <code>transform_query</code>).</li>\n",
    "                                </ul>\n",
    "                            </li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                    <li>If <strong>hallucination_grader</strong> returns <code>No</code>: Return <code>\"not supported\"</code> (route back to <code>generate</code> for regeneration).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b959ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a42df",
   "metadata": {},
   "source": [
    "## 2.4 Phase 2B: Web Search (If Query is unrelated to index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17444287",
   "metadata": {},
   "source": [
    "### 2.4.1 Description of Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978c116",
   "metadata": {},
   "source": [
    "<h2>Web Search Pipeline</h2>\n",
    "<p>The web search pipeline is utilized when the query is unrelated to the indexed data. It incorporates a web search tool to fetch relevant real-time results.</p>\n",
    "\n",
    "<h3>How it works:</h3>\n",
    "<ol>\n",
    "    <li>\n",
    "        <strong style=\"color:red\">Step 1: Building the Web Search Tool</strong>\n",
    "        <ul>\n",
    "            <li>The pipeline uses the <code>TavilySearchResults</code> tool, built on the Tavily API, which is specifically optimized for AI agents (LLMs).</li>\n",
    "            <li>It retrieves the top 3 search results (<code>k=3</code>) and delivers real-time, accurate, and factual information.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong style=\"color:red\">Step 2: Defining Graph Nodes as Functions</strong>\n",
    "        <ul>\n",
    "            <li>\n",
    "                <strong><code>web_search</code> Node:</strong>\n",
    "                <ul>\n",
    "                    <li><strong>Input:</strong> Current graph state containing the query (<code>state[\"question\"]</code>).</li>\n",
    "                    <li><strong>Operation:</strong>\n",
    "                        <ol>\n",
    "                            <li>Extract the query from the graph state.</li>\n",
    "                            <li>Pass the query to the <code>web_search_tool</code> to retrieve the top search results.</li>\n",
    "                            <li>Format the results into a <code>Document</code> object using the <code>langchain.schema.Document</code> class.</li>\n",
    "                        </ol>\n",
    "                    </li>\n",
    "                    <li><strong>Output:</strong> Updated graph state with the retrieved documents stored in the <code>state[\"documents\"]</code> key.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f8182",
   "metadata": {},
   "source": [
    "### 2.4.2 Step 1: Building the Web Search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d829bb-1074-4976-b650-ead41dcb9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ccbc8",
   "metadata": {},
   "source": [
    "### 2.4.3 Step 2: Defining Graph Nodes as Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b76b5ec3-0720-443d-85b1-c0e79659ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee156d",
   "metadata": {},
   "source": [
    "## 2.5 Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a7cef",
   "metadata": {},
   "source": [
    "<h1>Adaptive RAG Pipeline: Flow Construction Using <code>langgraph.graph</code></h1>\n",
    "\n",
    "<p>To implement the Adaptive Retrieval-Augmented Generation (RAG) process, we use the <code>langgraph.graph</code> library for efficient flow engineering. Here's how the pipeline is constructed:</p>\n",
    "\n",
    "<h2>1. Defining the Workflow</h2>\n",
    "<p>The pipeline begins with a <strong>StateGraph</strong>, which represents the entire workflow. This graph operates on a central state object (e.g., query, retrieved documents, generated responses). Nodes in the graph interact with this state, updating key-value pairs as operations progress.</p>\n",
    "\n",
    "<h2>2. Adding Nodes</h2>\n",
    "<p>Nodes represent distinct stages of the RAG pipeline. Each node corresponds to a predefined function or process:</p>\n",
    "<ul>\n",
    "    <li><strong><code>web_search</code>:</strong> Executes web searches for queries unrelated to the index.</li>\n",
    "    <li><strong><code>retrieve</code>:</strong> Fetches relevant documents from the vectorstore.</li>\n",
    "    <li><strong><code>grade_documents</code>:</strong> Evaluates the relevance of retrieved documents.</li>\n",
    "    <li><strong><code>generate</code>:</strong> Uses the LLM to generate answers based on the documents and query.</li>\n",
    "    <li><strong><code>transform_query</code>:</strong> Rewrites the query for better retrieval or regeneration.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>3. Adding Edges and Conditional Edges</h2>\n",
    "<p>Edges define the workflow sequence. There are two types of edges:</p>\n",
    "<ul>\n",
    "    <li><strong>Normal Edges:</strong> Always execute the next node in sequence. For example, after retrieving documents, grading their relevance always follows.</li>\n",
    "    <li><strong>Conditional Edges:</strong> Decide the next node based on certain conditions, typically evaluated by a routing function. For example:\n",
    "        <ul>\n",
    "            <li>After query analysis, route the query to either <code>web_search</code> or <code>retrieve</code>.</li>\n",
    "            <li>After document grading, decide whether to generate an answer or transform the query.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<h2>Pipeline Flow Description</h2>\n",
    "<ol>\n",
    "    <li><strong>Start of Workflow:</strong> \n",
    "        <ul>\n",
    "            <li>The graph begins with a query. A routing function (<code>route_question</code>) determines the first step:</li>\n",
    "            <ul>\n",
    "                <li>If the query is unrelated to the index, route to <code>web_search</code>.</li>\n",
    "                <li>If related, route to <code>retrieve</code>.</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Web Search Node:</strong> \n",
    "        <ul>\n",
    "            <li>If routed to <code>web_search</code>, the node fetches the top web results and sends them to the <code>generate</code> node.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Retrieve Node:</strong> \n",
    "        <ul>\n",
    "            <li>Fetch relevant documents from the vectorstore, then pass them to the <code>grade_documents</code> node.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Document Grading Node:</strong> \n",
    "        <ul>\n",
    "            <li>Filter irrelevant documents. Based on the filtered results:</li>\n",
    "            <ul>\n",
    "                <li>If no relevant documents remain, route to <code>transform_query</code>.</li>\n",
    "                <li>If relevant documents are present, route to <code>generate</code>.</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Query Transformation Node:</strong> \n",
    "        <ul>\n",
    "            <li>Rewrite the query to improve relevance, then send it back to the <code>retrieve</code> node.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Generation Node:</strong> \n",
    "        <ul>\n",
    "            <li>Generate an answer using the documents and query. Evaluate the generation with:</li>\n",
    "            <ul>\n",
    "                <li><strong>Hallucination Grader:</strong> Checks if the answer is grounded in retrieved documents.</li>\n",
    "                <li><strong>Answer Grader:</strong> Ensures the answer addresses the query.</li>\n",
    "                <li>If both pass, the workflow ends with the generated response.</li>\n",
    "                <li>If not, regenerate (hallucination) or transform the query(answer did not address question).</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<p>This pipeline dynamically adjusts to query complexity, ensuring efficient and accurate responses by combining retrieval-augmented techniques and adaptive query processing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67854e07-9293-4c3c-bf9a-bc9a605570ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bce541",
   "metadata": {},
   "source": [
    "## 2.6 Use Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04289f28",
   "metadata": {},
   "source": [
    " <h3>1. Inputs and Initialization</h3>\n",
    "<p>The pipeline starts with an input dictionary containing the userâ€™s question.</p>\n",
    "<ul>\n",
    "    <li><strong>Example Question 1:</strong> \"What player at the Bears expected to draft first in the 2024 NFL draft?\"</li>\n",
    "    <li><strong>Example Question 2:</strong> \"What are the types of agent memory?\"</li>\n",
    "</ul>\n",
    "\n",
    "<h3>2. Streaming Execution of the Graph</h3>\n",
    "<p><code>app.stream(inputs)</code> iteratively executes the nodes in the state graph. At each iteration:</p>\n",
    "<ul>\n",
    "    <li>The current node being executed is identified.</li>\n",
    "    <li>The nodeâ€™s name and the state after execution are printed.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>3. Routing Questions</h3>\n",
    "<p><strong>Routing Node:</strong> The first step in the pipeline determines whether the query should be handled by:</p>\n",
    "<ul>\n",
    "    <li><strong>Web Search:</strong> For questions unrelated to indexed documents.</li>\n",
    "    <li><strong>RAG:</strong> For questions that align with the indexed data.</li>\n",
    "</ul>\n",
    "<p>Based on the routing decision, the next node is invoked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29acc541-d726-4b75-84d1-a215845fe88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Caleb Williams from USC is expected to be drafted first by the Bears in the '\n",
      " '2024 NFL draft.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"What player at the Bears expected to draft first in the 2024 NFL draft?\"\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fddd00-58bf-4910-bf36-be9e5bfba778",
   "metadata": {},
   "source": [
    "Trace on LangSmith:https://smith.langchain.com/public/96c1df71-b9ee-4ffd-88de-b01e2974d6b2/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69a985dd-03c6-45af-a67b-b15746a2cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('The types of agent memory are short-term memory, which is utilized for '\n",
      " 'in-context learning and is restricted by the context window length, and '\n",
      " 'long-term memory, which involves an external vector store for storing and '\n",
      " 'recalling information over extended periods. Sensory memory is also '\n",
      " 'mentioned as the earliest stage of memory, retaining sensory impressions '\n",
      " 'briefly after stimuli end.')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf41097-fc4c-4072-95b3-e7e07731ada1",
   "metadata": {},
   "source": [
    "Trace on LangSmith:https://smith.langchain.com/public/b833d773-47d3-4e5d-b385-6ef1ba3bce49/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
